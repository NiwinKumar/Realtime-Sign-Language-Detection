# Realtime Sign Language Detection Using LSTM Model

![Mediapipe Detection](https://github.com/NiwinKumar/Realtime-Sign-Language-Detection/assets/32614982/4d6a7542-2409-424f-9c9f-6b90a3236ea6)

> The Realtime Sign Language Detection Using LSTM Model is a deep learning-based project designed to recognize and interpret sign language gestures in real-time. It leverages Long Short-Term Memory (LSTM) neural network architecture for accurate detection and classification of gestures captured from a video feed. This system offers a user-friendly interface where users can perform sign language gestures in front of a camera, and the system provides instant detection and interpretation. It serves as an assistive technology to enhance communication for individuals with hearing impairments. Key features include real-time gesture detection, high accuracy, and the ability to customize and train new sign language gestures. Built using Python, TensorFlow, OpenCV, and Numpy, this project is accessible and highly adaptable, aiming to bridge the communication gap and empower the deaf and hard-of-hearing community.

## Table of Contents

- [About the Project](#about-the-project)
- [Demo](#demo)
- [Features](#features)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Contributing](#contributing)
- [Contact](#contact)

## About the Project

This project aims to develop a robust system that accurately detects and interprets sign language gestures in real-time. It utilizes LSTM (Long Short-Term Memory) models to achieve high accuracy and adaptability. By enhancing communication accessibility, this project supports individuals with hearing impairments and contributes to inclusivity.

## Demo

A demonstration of the Realtime Sign Language Detection Using LSTM Model is provided to showcase its capabilities.

https://github.com/NiwinKumar/Realtime-Sign-Language-Detection/assets/32614982/16bd1d47-cc3f-488c-8d0e-e400004dc716

The demo illustrates how the system accurately interprets sign language gestures and delivers real-time results.

## Features

![model h5](https://github.com/NiwinKumar/Realtime-Sign-Language-Detection/assets/32614982/ece8ef5e-295c-4cfd-beb5-255ea88c8b76)

- **Real-time Detection**: Instantly detects and interprets sign language gestures.
- **High Accuracy**: Utilizes an LSTM model for precise recognition of diverse gestures.
- **Multi-Gesture Support**: Recognizes various sign language gestures for effective communication.
- **Easy Integration**: Provides examples and code snippets for seamless integration into other applications.
- **Accessibility**: Enhances communication accessibility for the deaf and hard-of-hearing community.
- **Customizable**: Supports gesture customization to meet specific user needs.
- **Language Adaptability**: Trainable for different sign languages, expanding its usability across regions.
- **User-Friendly Interface**: Simple and intuitive interaction for smooth user experience.
- **Open Source**: Encourages collaboration and contributions from the development community.

## Getting Started

To set up and use the Realtime Sign Language Detection system, follow these steps:

### Prerequisites

- Python
- TensorFlow
- OpenCV
- Numpy

### Installation

1. Clone the repository:

```shell
git clone https://github.com/NiwinKumar/Realtime-Sign-Language-Detection.git
```

2. Install Dependencies:

```shell
pip install notebook
```

3. Run Jupyter Notebook:

```shell
jupyter notebook
```

## Usage

Open the 'RealTimeSignLanguageDetection.ipynb' file in Jupyter Notebook and execute all the cells to start the detection system.

## Contributing

Contributions are highly encouraged! If you have suggestions, ideas, or bug fixes, please open an issue or submit a pull request.

## Contact

For any inquiries or support, reach out at niwinkumar7@gmail.com.

